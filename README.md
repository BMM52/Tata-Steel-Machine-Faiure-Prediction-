# ğŸ­ Tata Steel â€“ Machine Failure Prediction using Machine Learning

<p align="center">
  <img src="https://img.shields.io/badge/Machine%20Learning-Predictive%20Maintenance-blue?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Model-XGBoost-success?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Status-Completed-brightgreen?style=for-the-badge"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10-blue"/>
  <img src="https://img.shields.io/badge/Scikit--Learn-ML-orange"/>
  <img src="https://img.shields.io/badge/XGBoost-Boosting-red"/>
  <img src="https://img.shields.io/badge/SHAP-Explainability-purple"/>
  <img src="https://img.shields.io/badge/Google%20Colab-Notebook-yellow"/>
</p>

---

## ğŸ“Œ Project Overview

Unexpected machine failures in large-scale steel manufacturing can cause:

- â±ï¸ Unplanned production downtime  
- ğŸ’° High repair and maintenance costs  
- âš ï¸ Safety risks for operators  

This project builds a **machine learningâ€“based predictive maintenance system** for **Tata Steel**, capable of predicting machine failures **before they occur**, enabling **proactive maintenance decisions**.

The dataset is synthetically generated but closely reflects **real-world industrial machine behavior**.

---

## ğŸ¯ Business Objective

- Predict **machine failure (binary classification)**
- Minimize **false negatives (missed failures)**
- Enable **early intervention**
- Improve **Overall Equipment Effectiveness (OEE)**
- Reduce **operational cost and safety risk**

---

## ğŸ“‚ Dataset Description

### ğŸ“Š Data Files
- **Training Dataset (`train.csv`)**
  - 136,429 records
  - Includes target variable
- **Test Dataset (`test.csv`)**
  - 90,954 records
  - Used for final predictions

### ğŸ§¾ Key Features
- ğŸŒ¡ï¸ Air temperature  
- ğŸ”¥ Process temperature  
- âš™ï¸ Rotational speed  
- ğŸ§² Torque  
- ğŸ› ï¸ Tool wear  
- ğŸ·ï¸ Machine type (categorical)  
- ğŸš¨ Failure indicators (TWF, HDF, PWF, OSF, RNF)

### ğŸ¯ Target Variable
- **Machine failure**
  - `1` â†’ Failure occurred  
  - `0` â†’ No failure  

> âš ï¸ **Highly Imbalanced Dataset**  
> Only ~**1.57%** of observations correspond to failures.

---

## ğŸ” Exploratory Data Analysis (EDA)

Key EDA steps performed:

- âœ” Missing value and duplicate checks  
- ğŸ“ˆ Feature distribution analysis  
- ğŸ” Failure class imbalance visualization  
- ğŸ”¥ Correlation heatmaps  
- âš ï¸ Failure-type relationship analysis  
- ğŸ“‰ Outlier detection using **Z-score** and **IQR methods**

---

## ğŸ› ï¸ Feature Engineering

Domain-driven features were engineered to better capture machine stress patterns:

| Feature | Description |
|-------|------------|
| `temp_diff` | Process temperature âˆ’ Air temperature |
| `torque_per_rpm` | Torque / Rotational speed |
| `temp_ratio` | Process temperature / Air temperature |
| `temp_interaction` | Process Ã— Air temperature |
| `high_wear_flag` | 1 if Tool wear > 150 |

These features significantly improved predictive performance.

---

## âš™ï¸ Preprocessing Pipeline

- ğŸ“ **StandardScaler** for numerical features  
- ğŸ·ï¸ **OneHotEncoder** for machine type  
- ğŸ”€ **Trainâ€“Validation Split**
  - 80% Train / 20% Validation  
  - Stratified to preserve class imbalance  

---

## ğŸ¤– Model Building & Comparison

### ğŸ” Models Evaluated
- Logistic Regression  
- Random Forest  
- XGBoost (baseline)

### ğŸ“Š Evaluation Metrics
- Accuracy  
- Precision  
- Recall  
- F1-score  
- **ROC-AUC (primary metric)**

| Model | ROC-AUC | Recall | Precision |
|------|--------|--------|-----------|
| Logistic Regression | 0.87 | High | Very Low |
| Random Forest | 0.91 | Medium | Medium |
| â­ **XGBoost (Baseline)** | **0.915** | **High** | **Best Balance** |

âœ… **XGBoost was selected for advanced tuning**.

---

## ğŸš€ Hyperparameter Tuning

- Performed using **RandomizedSearchCV**
- Tuned parameters include:
  - `max_depth`
  - `learning_rate`
  - `n_estimators`
  - `subsample`
  - `colsample_bytree`
  - `scale_pos_weight`

ğŸ¯ **Best ROC-AUC achieved:** **~0.9366**

---

## ğŸšï¸ Threshold Optimization

The default probability threshold (0.5) is unsuitable for highly imbalanced data.

A threshold sweep from **0.10 â†’ 0.90** was conducted.

### âœ… Final Threshold: **0.40**

- **Recall:** ~81%
- **Precision:** ~12%
- **ROC-AUC:** ~0.92

> In industrial environments, **missing a failure is far more costly** than investigating a false alarm â€” hence recall is prioritized.

---

## ğŸ§  Model Explainability (SHAP)

SHAP was used to interpret model predictions.

### ğŸ”‘ Most Influential Features:
- Torque per RPM  
- Temperature Difference  
- Tool Wear  
- Process Temperature  
- Machine Type  

This ensures **model transparency and trust**, critical for industrial deployment.

---

## ğŸ“ˆ Final Model & Predictions

- Final XGBoost model trained on **100% of training data**
- Predictions generated for unseen test data
- Threshold = 0.40 applied
- Final output contains:
  - `id`
  - `Machine failure`

---

## ğŸ’¼ Business Impact

Implementing this predictive maintenance system enables:

- ğŸ”» Reduced unplanned downtime  
- ğŸ’° Lower maintenance and repair costs  
- âš™ï¸ Optimized maintenance scheduling  
- ğŸ“ˆ Improved Overall Equipment Effectiveness (OEE)  
- ğŸ¦º Enhanced operator safety  

ğŸ’¡ Preventing even **one major failure per month** can save **lakhs of rupees** in operational losses.

---

## ğŸ”® Future Enhancements

- Real-time sensor data streaming  
- Vibration and acoustic data integration  
- REST API deployment (FastAPI / Flask)  
- Live machine health monitoring dashboard  
- Continuous model retraining

---

## ğŸ§‘â€ğŸ’» Tech Stack

- ğŸ Python  
- ğŸ“Š Pandas, NumPy  
- ğŸ¤– Scikit-Learn  
- ğŸš€ XGBoost  
- ğŸ§  SHAP  
- ğŸ“ˆ Matplotlib, Seaborn  
- â˜ï¸ Google Colab  

---

## ğŸ‘¤ Author

**Burhanuddin Motiwala**  
ğŸ“Š Aspiring Data Scientist | Machine Learning Enthusiast  

ğŸ”— **GitHub:** https://github.com/burhanuddinmo  
ğŸ”— **LinkedIn:** https://www.linkedin.com/in/burhanuddinmotiwala  



